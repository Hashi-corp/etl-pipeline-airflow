# ETL Pipeline with Apache Airflow

This repository focusses on building a basic ETL pipeline using **Apache Airflow** for monitoring and automating workflows.

---

## âœ¨ Features

- Weather monitoring for a given location
- Custom Airflow DAG for ETL operations
- Fully Dockerized setup with Airflow Web UI

---

## ğŸ“ Project Structure

```
etl-pipeline-airflow/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_pipeline.py          
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     
â”‚   â””â”€â”€ processed/               
â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ custom_operators/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ extract_operator.py  
â”‚       â”œâ”€â”€ transform_operator.py
â”‚       â””â”€â”€ load_operator.py    
â”œâ”€â”€ docker-compose.yml          
â”œâ”€â”€ requirements.txt            
â””â”€â”€ README.md                    
```

---

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/Hashi-corp/etl-pipeline-airflow.git
cd etl-pipeline-airflow
```

### 2. Start Airflow using Docker Compose

```bash
docker-compose up airflow-init
docker-compose up
```

- ğŸŒ Access Airflow UI: http://localhost:8080  
- ğŸ” Default login:
  - Username: `airflow`
  - Password: `airflow`

### 3. Trigger the DAG

- Open the Airflow UI
- Enable and trigger the `etl_pipeline` DAG manually, or let it run on its schedule (daily)

---

## ğŸ“¦ Requirements

If not using Docker, install dependencies manually:

```bash
pip install -r requirements.txt
```

Core dependencies:

- `apache-airflow`
- `docker` (optional for containerization)

---

## ğŸ“„ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.
